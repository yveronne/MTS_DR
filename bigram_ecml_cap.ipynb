{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"name":"bigram_ecml_cap.ipynb","provenance":[],"collapsed_sections":["8HgXCZpPBLrY"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"HC0DNTF0BLqx","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","import pandas as pd\n","import numpy as np\n","from timeit import default_timer as timer\n","import scipy.io as sio\n","from scipy.special import comb\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","from sklearn.svm import SVC\n","from scipy.stats import expon\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.manifold import spectral_embedding\n","import math\n","\n","\n","import ensemble_learning as el"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P0wo7phzBLq9"},"source":["# Functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UldG0q3SBLq-","colab":{}},"source":["def onegramsTransform(data,dim1,dim2,minb=2,maxbh=30,cv = 5):\n","    #Removing missing values and store the result in the same variable\n","    data.dropna(inplace=True)\n","    #Grouping data by series\n","    group = data.groupby('Series')\n","    #Getting the different classes\n","    classe = group['classe'].apply(lambda x : x.iloc[0]).reset_index(drop=True)\n","    #Providing train/test indices to split data in train/test sets. Split dataset into cv consecutive folds, and shuffling the data before it\n","    kf = KFold(n_splits=cv,shuffle=True)\n","    mscore = 0\n","    #b1 from 2 to 29...by default\n","    for b1 in range(minb,maxbh):\n","        hist = group.apply(lambda x : np.ndarray.tolist(np.ndarray.flatten(\n","                    np.histogram(x.iloc[:,dim1].values,bins=b1,density=True)[0])))\n","        hist_tr = pd.DataFrame(hist.values.tolist())\n","\n","        clf= KNeighborsClassifier(1)\n","        clf.fit(hist_tr, classe)\n","\n","        scores = cross_val_score(clf, hist_tr, classe, cv=kf)\n","\n","        if mscore < np.median(scores):\n","            \n","            mscore = np.median(scores)\n","            ubh1=b1\n","            \n","    #print(\"Bins: \", [ubh1], \" \\nMean scores : \", mscore)       \n","            \n","    mscore=0\n","    for b1 in range(minb,maxbh):\n","        hist = group.apply(lambda x : np.ndarray.tolist(np.ndarray.flatten(\n","                    np.histogram(x.iloc[:,dim2].values,bins=b1,density=True)[0])))\n","        hist_tr = pd.DataFrame(hist.values.tolist())\n","\n","        clf=KNeighborsClassifier(1)\n","        clf.fit(hist_tr, classe)\n","\n","        scores = cross_val_score(clf, hist_tr, classe, cv=kf)\n","\n","        if mscore < np.median(scores):\n","            \n","            mscore = np.median(scores)\n","            ubh2=b1\n","\n","    #print(\"Bins: \", [ubh2], \" \\nMean scores : \", mscore)\n","\n","    return ubh1,ubh2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VR8WMCp6BLrC","colab":{}},"source":["def bigramsTransform(data,dim1,dim2,minb=2,maxb=11,cv = 5):\n","    data.dropna(inplace=True)\n","    group = data.groupby('Series')\n","    classe = group['classe'].apply(lambda x : x.iloc[0]).reset_index(drop=True)\n","    kf = KFold(n_splits=cv,shuffle=True)\n","    mscore = 0\n","    for b1 in range(minb,maxb):\n","        for b2 in range(minb,maxb):\n","            hist = group.apply(lambda x : np.ndarray.tolist(np.ndarray.flatten(\n","                        np.histogram2d(x.iloc[:,dim1].values,x.iloc[:,dim2].values,bins=[b1,b2],density=True)[0])))\n","            hist_tr = pd.DataFrame(hist.values.tolist())\n","\n","            clf= KNeighborsClassifier(1)\n","            clf.fit(hist_tr, classe)\n","\n","            scores = cross_val_score(clf, hist_tr, classe, cv=kf)\n","\n","            if mscore < np.median(scores):\n","                \n","                mscore = np.median(scores)\n","                ub1=b1\n","                ub2=b2\n","    #print(\"Bins: \", [ub1,ub2], \" \\nMean scores : \", mscore) \n","    return ub1,ub2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AZMg3K6NBLrH"},"source":["# Datasets"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VIxwmaZwBLrI"},"source":["### Text"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NKXt8C2fBLrJ","colab":{}},"source":["dataset = [[\"ArabicDigits/ArabicDigits\",\" \"],#0\n","           [\"AUSLAN/AUSLAN\",\" \",\"\\t\"],#1\n","           [\"CharacterTrajectories/CharacterTrajectories\",\"\\t\"], #2          \n","           [\"CMUsubject16/CMUsubject16\",\"\\t\"],#3\n","           [\"ECG/ECG\",\" \"],#4\n","           [\"Libras/Libras\",\" \"],#5\n","           [\"PenDigits/PenDigits\",\"\\t\"],#6\n","           ['UWave/uWave',\" \"],#7\n","           ['RobotFailure/LP1',\" \"],#8\n","           ['RobotFailure/LP2',\" \"],#9\n","           ['RobotFailure/LP3',\" \"],#10\n","           ['RobotFailure/LP4',\" \"],#11\n","           ['RobotFailure/LP5',\" \"],#12\n","           ['Wafer/Wafer',\" \"],#13\n","           [\"JapaneseVowels/JapaneseVowels\", \" \"], #14\n","           [\"ArticularyWordRecognition/ArticularyWordRecognition\", \" \"],#15\n","           [\"BasicMotions/BasicMotions\", \" \"], #16\n","           [\"Cricket/Cricket\", \" \"], #17\n","           [\"DuckDuckGeese/DuckDuckGeese\", \" \"], #18\n","           [\"EigenWorms/EigenWorms\", \" \"], #19\n","           [\"FingerMovements/FingerMovements\", \" \"], #20\n","           [\"HandMovementDirection/HandMovementDirection\", \" \"], #21\n","           [\"Heartbeat/Heartbeat\", \" \"], #22\n","           [\"LSST/LSST\", \" \"], #23\n","           [\"MotorImagery/MotorImagery\", \" \"], #24\n","           [\"NATOPS/NATOPS\", \" \"], #25\n","           [\"RacketSports/RacketSports\", \" \"], #26\n","           [\"SelfRegulationSCP1/SelfRegulationSCP1\", \" \"], #27\n","           [\"SelfRegulationSCP2/SelfRegulationsSCP2\", \" \"] #28\n","              ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QDyiM_6dBLrM","colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":554,"status":"ok","timestamp":1594298911437,"user":{"displayName":"Véronne Yepmo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqH82zY4SlxjzAyvHbBobvn-Nfd4c_b3i6F_ADfA=s64","userId":"01798836279637929908"},"user_tz":-120},"outputId":"3634cecf-0406-4efb-ee11-a2c077b762ce"},"source":["i=21\n","\n","train = pd.read_table(\"MTS_Datasets/\"+dataset[i][0]+\"_TRAIN\",sep=dataset[i][1],header=None)\n","\n","\n","# Renommage des 3 premières colonnes représentant resp. les numéros des STM,\n","# les indices de leurs obs et leur classe\n","train.rename(index=str, columns={0: \"Series\", 1: \"index\", 2:\"classe\"},inplace=True)\n","# Suppression de la colonne des index des obs des STM, infos inutiles pour les futures traitements\n","# (d'où l'utilisation de inplace=True) sinon le dataframe appelant la méthode drop serait inchangé et \n","# une copie modifiée serait retournée en résultat\n","train.drop(['index'],axis=1,inplace=True) \n","# Suppression et sauvegarde de la colonne des classes et celle des séries\n","cl = train.pop('classe') \n","se = train.pop('Series')\n","# Insertion des colonnes précédemment supprimées à la fin du dataframe\n","train['classe'] = cl\n","train['Series'] = se\n","# Convertion du type des colonnes en string\n","train.columns = train.columns.astype(str)\n","print(train.head())\n","\n","\n","test = pd.read_table(\"MTS_Datasets/\"+dataset[i][0]+\"_TEST\",sep=dataset[i][1],header=None)\n","test.rename(index=str, columns={0: \"Series\", 1: \"index\", 2:\"classe\"},inplace=True)\n","test.drop(['index'],axis=1,inplace=True)\n","cl = test.pop('classe')\n","se = test.pop('Series')\n","test['classe'] = cl\n","test['Series'] = se\n","test.columns = test.columns.astype(str)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["            3          4          5           6           7           8  \\\n","0  152.472131  12.392351  18.801367  119.067984    9.371185  176.878290   \n","1  151.149948  77.166998   7.669297   92.378960   28.596053  134.774572   \n","2  133.185425  75.533099  20.021506   50.618334   33.540720   77.921054   \n","3  105.026618  78.709435  -8.449302  -24.030543  -80.624318    8.100860   \n","4   96.332784  90.910153 -62.002411  -36.308074 -163.957234   14.436097   \n","\n","            9          10          11          12  classe  Series  \n","0  187.217536  148.597142  192.406885  125.757108       1       1  \n","1  181.612639   -7.859846  123.777559  -38.720199       1       1  \n","2  143.941572  -75.591813   37.682789  -86.323079       1       1  \n","3   -8.680976 -132.255432   -1.438948  -37.793645       1       1  \n","4  -63.609558 -136.074317   84.123071   -3.151767       1       1  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","heading_collapsed":true,"id":"8HgXCZpPBLrY"},"source":["### Matlab"]},{"cell_type":"code","metadata":{"colab_type":"code","hidden":true,"id":"BzwRPnTZBLrZ","colab":{}},"source":["mat = sio.loadmat('MTS_Datasets/PEMS/PEMS.mat')\n","mdata = mat['mts']\n","mdtype = mdata.dtype  \n","ndata = {n: mdata[n][0, 0] for n in mdtype.names}\n","columns = [n for n, v in ndata.items()]\n","\n","c='train'\n","train = pd.DataFrame()\n","for i in range(0,len(ndata[c][0])):\n","    series = pd.DataFrame(np.transpose(ndata[c][0][i]))\n","    series['classe']=ndata['trainlabels'][0][i]\n","    series['Series']=i\n","    train = pd.concat([train,series])\n","train.reset_index(drop=True,inplace=True)\n","\n","\n","c='test'\n","test = pd.DataFrame()\n","for i in range(0,len(ndata[c][0])):\n","    series = pd.DataFrame(np.transpose(ndata[c][0][i]))\n","    series['classe']=ndata['testlabels'][0][i]\n","    series['Series']=i\n","    test = pd.concat([test,series])\n","test.reset_index(drop=True,inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","hidden":true,"id":"4qupnhphBLrd","colab":{}},"source":["mat = sio.loadmat('MTS_Datasets/JapaneseVowels/JapaneseVowels.mat')\n","mdata = mat['mts']\n","mdtype = mdata.dtype  \n","ndata = {n: mdata[n][0, 0] for n in mdtype.names}\n","columns = [n for n, v in ndata.items()]\n","\n","c='train'\n","train = pd.DataFrame()\n","for i in range(0,len(ndata[c][0])):\n","    series = pd.DataFrame(np.transpose(ndata[c][0][i]))\n","    series['classe']=ndata['trainlabels'][i][0]  # if Japanese Vowels\n","    #series['classe']=ndata['trainlabels'][0][i]\n","    series['Series']=i\n","    train = pd.concat([train,series])\n","train.reset_index(drop=True,inplace=True)\n","\n","\n","c='test'\n","test = pd.DataFrame()\n","for i in range(0,len(ndata[c][0])):\n","    series = pd.DataFrame(np.transpose(ndata[c][0][i]))\n","    series['classe']=ndata['testlabels'][i][0]     # if Japanese Vowels\n","    #series['classe']=ndata['testlabels'][0][i]\n","    series['Series']=i\n","    test = pd.concat([test,series])\n","test.reset_index(drop=True,inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RHyLm0aMBLrh"},"source":["# Pre_treatments"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9u8ikRi7BLrh","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1594298916621,"user":{"displayName":"Véronne Yepmo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqH82zY4SlxjzAyvHbBobvn-Nfd4c_b3i6F_ADfA=s64","userId":"01798836279637929908"},"user_tz":-120},"outputId":"6330622e-392b-4a39-9b3f-f323dc788fbf"},"source":["# Détermination du nbre d'instances ou observations composant (les dimensions des) les STM\n","nb_col = len(train.drop(['classe','Series'],axis=1).columns)\n","print(nb_col)\n","print(train.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10\n","            3          4          5           6           7           8  \\\n","0  152.472131  12.392351  18.801367  119.067984    9.371185  176.878290   \n","1  151.149948  77.166998   7.669297   92.378960   28.596053  134.774572   \n","2  133.185425  75.533099  20.021506   50.618334   33.540720   77.921054   \n","3  105.026618  78.709435  -8.449302  -24.030543  -80.624318    8.100860   \n","4   96.332784  90.910153 -62.002411  -36.308074 -163.957234   14.436097   \n","\n","            9          10          11          12  classe  Series  \n","0  187.217536  148.597142  192.406885  125.757108       1       1  \n","1  181.612639   -7.859846  123.777559  -38.720199       1       1  \n","2  143.941572  -75.591813   37.682789  -86.323079       1       1  \n","3   -8.680976 -132.255432   -1.438948  -37.793645       1       1  \n","4  -63.609558 -136.074317   84.123071   -3.151767       1       1  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XIUvmNiNBLrl"},"source":["## Normalisation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"URyKRM4zBLrm","scrolled":true,"colab":{}},"source":["train.iloc[:,0:nb_col] = train.groupby('Series').transform(lambda x: (x - x.min()) / (x.max()-x.min())).iloc[:,0:nb_col]\n","test.iloc[:,0:nb_col] = test.groupby('Series').transform(lambda x: (x - x.min()) / (x.max()-x.min())).iloc[:,0:nb_col]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"70xVH7VPKPtS","colab_type":"code","colab":{},"outputId":"8f8e47a1-7e0e-4159-d5a5-c29bb6a02b55"},"source":["print(train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              3         4         5         6         7         8         9  \\\n","0      0.754349  0.666922  0.617529  0.854880  0.613323  0.833209  0.773135   \n","1      0.752683  0.748586  0.605668  0.817689  0.635159  0.778454  0.767421   \n","2      0.730046  0.746526  0.618829  0.759494  0.640775  0.704519  0.729021   \n","3      0.694564  0.750531  0.588494  0.655469  0.511106  0.613720  0.573445   \n","4      0.683609  0.765913  0.531435  0.638360  0.416456  0.621959  0.517454   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","63995  0.503725  0.335637  0.457429  0.480824  0.564866  0.493387  0.672866   \n","63996  0.607420  0.376014  0.482064  0.518447  0.692324  0.708024  0.799360   \n","63997  0.627006  0.384698  0.505556  0.531257  0.693084  0.761330  0.807695   \n","63998  0.617961  0.385621  0.470202  0.562756  0.582686  0.738414  0.694918   \n","63999  0.596075  0.382868  0.396359  0.549316  0.501977  0.694901  0.609682   \n","\n","             10        11        12  classe  Series  \n","0      0.759200  0.798597  0.804658       1       1  \n","1      0.519901  0.715486  0.585187       1       1  \n","2      0.416306  0.611224  0.521668       1       1  \n","3      0.329640  0.563847  0.586424       1       1  \n","4      0.323800  0.667464  0.632648       1       1  \n","...         ...       ...       ...     ...     ...  \n","63995  0.487788  0.606696  0.618497       4     160  \n","63996  0.706778  0.744411  0.864981       4     160  \n","63997  0.768254  0.758342  0.940002       4     160  \n","63998  0.786203  0.779814  0.892783       4     160  \n","63999  0.770835  0.816389  0.810330       4     160  \n","\n","[64000 rows x 12 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JvkrzDFJBLrq","colab":{}},"source":["mask  = train.isna().all()\n","\n","#Drop columns where all elements are missing and keep the dataframe with valid entries in the same variable\n","train.dropna(axis=1,how='all',inplace=True)\n","\n","train.interpolate(inplace=True)\n","train.dropna(inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U_iC7I4uBLru","colab":{}},"source":["test = test[test.columns[~mask]]\n","test.dropna(axis=1,how='all',inplace=True)\n","test.interpolate(inplace=True)\n","test.dropna(inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8DhQSPfKPty","colab_type":"code","colab":{},"outputId":"74959552-9ebe-46e9-a9ce-7dad0f5da75b"},"source":["print(train.head(401))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["            3         4         5         6         7         8         9  \\\n","0    0.754349  0.666922  0.617529  0.854880  0.613323  0.833209  0.773135   \n","1    0.752683  0.748586  0.605668  0.817689  0.635159  0.778454  0.767421   \n","2    0.730046  0.746526  0.618829  0.759494  0.640775  0.704519  0.729021   \n","3    0.694564  0.750531  0.588494  0.655469  0.511106  0.613720  0.573445   \n","4    0.683609  0.765913  0.531435  0.638360  0.416456  0.621959  0.517454   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","396  0.553296  0.748621  0.637124  0.918330  0.703182  0.773564  0.609546   \n","397  0.565759  0.739280  0.648544  0.925413  0.744639  0.738139  0.580484   \n","398  0.630152  0.732568  0.767051  0.927562  0.879331  0.712392  0.588253   \n","399  0.598498  0.737850  0.815125  0.917400  0.904102  0.697302  0.607643   \n","400  0.422042  0.441776  0.543537  0.368657  0.571177  0.269917  0.512898   \n","\n","           10        11        12  classe  Series  \n","0    0.759200  0.798597  0.804658       1       1  \n","1    0.519901  0.715486  0.585187       1       1  \n","2    0.416306  0.611224  0.521668       1       1  \n","3    0.329640  0.563847  0.586424       1       1  \n","4    0.323800  0.667464  0.632648       1       1  \n","..        ...       ...       ...     ...     ...  \n","396  0.600527  0.463993  0.789701       1       1  \n","397  0.616219  0.474094  0.771397       1       1  \n","398  0.648287  0.551015  0.746276       1       1  \n","399  0.620179  0.569420  0.717859       1       1  \n","400  0.349797  0.343074  0.265043       1       2  \n","\n","[401 rows x 12 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XGwP_ogjKPt4","colab_type":"raw"},"source":["Saving the first serie into csv file to compute intrinsic dimensionality"]},{"cell_type":"code","metadata":{"id":"xqi4oDZ_KPt5","colab_type":"code","colab":{}},"source":["train.iloc[:400, 0:10].to_csv(\"handMovementDirection.csv\", index=False , header=False,sep=\" \")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8UqxTyFqikHX"},"source":["# Manifold learning"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kL5P73v-pYuZ","colab":{},"outputId":"d3d196ca-305d-401d-b8a0-fb70a4db2ce6"},"source":["grouped_df = train.groupby(\"Series\")\n","aaaaa = []\n","for key, item in grouped_df:\n","    #print(grouped_df.get_group(key).shape[0], \"\\n\\n\")\n","    aaaaa.append(grouped_df.get_group(key).shape[0])\n","\n","aaaaa.sort()\n","print(aaaaa)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"E6qngCN5Ofq5","colab":{}},"source":["!pip install tslearn\n","from tslearn.metrics import dtw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6U-EquDlioXf","colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":1925,"status":"ok","timestamp":1594298961144,"user":{"displayName":"Véronne Yepmo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqH82zY4SlxjzAyvHbBobvn-Nfd4c_b3i6F_ADfA=s64","userId":"01798836279637929908"},"user_tz":-120},"outputId":"6580a6c8-4270-4f70-88e0-727fe3a0eccf"},"source":["def reduce(data, nbcol, dim=3, repetition=False):\n","  # data is a dataframe representing a serie of the dataset\n","  size = data.shape[0]\n","    \n","  # If the number or data points if less or equal to the reduced dimension wished, drop the series\n","  if dim >= size:\n","    data.drop(data.index, inplace=True)\n","    return data\n","  #if size > 25:\n","  #  repetition = True\n","    \n","\n","  A = np.zeros((size, size))\n","  np.fill_diagonal(A, 1) \n","  R = np.zeros((size, size)) # Repetition neighbourhood matrix\n","  \n","\n","  # Handling the extreme data points\n","  A[0,1] = math.exp(-np.linalg.norm(data.iloc[0,:nbcol] - data.iloc[1,:nbcol]))\n","  A[1,0] = A[0,1]\n","  end = size - 1 # Last index of A\n","  bend = end - 1\n","  A[end,bend] = math.exp(-np.linalg.norm(data.iloc[end,:nbcol] - data.iloc[bend,:nbcol]))\n","  A[bend,end] = A[end,bend]\n","\n","  if repetition == True: # We compute simultaneously the adjacent temporal neighbourhood matrix and the repetition temporal neighbourhood matrix\n","\n","    endRepetition = end-10 # Last index of the data points while computing the repetition neighbourhoods\n","    fragmentNumbers = endRepetition - 9 # Number of fragments\n","    M = np.zeros((fragmentNumbers, fragmentNumbers)) # Similarity matrix of the fragments\n","    fragments = list() # List containing the fragments. Each element of the list contains the coordinates (extracted dataframe) of the data points in the fragment\n","    M_mod = np.zeros((fragmentNumbers, fragmentNumbers)) # Similarity matrix M after windowing\n","\n","    # Handling the intermediate data points for adjacent neighbours\n","    for i in range(1,end):\n","      A[i,i-1] =  math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i-1,:nbcol]))\n","      A[i-1,i] = A[i,i-1]\n","      A[i,i+1] = math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i+1,:nbcol]))\n","      A[i+1,i] = A[i,i+1]\n","      if i >= 10 and i <= endRepetition:\n","        fragments.append(data.iloc[i-10:i+11, :nbcol])\n","\n","    # Computing the M matrix\n","    for i in range(0,fragmentNumbers):\n","      for j in range(0,i):\n","        M[i,j] = dtw(fragments[i], fragments[j])\n","        M[j,i] = M[i,j]\n","\n","    # Performing temporal windowing of M\n","    for i in range(0,fragmentNumbers):\n","      for j in range(i+1, fragmentNumbers):\n","        for b in range(0, 20):\n","          if i-b > 0 and j-b >0:  #  ET LES AUTRES ?\n","            M_mod[i,j] = M_mod[i,j] + M[i-b,j-b]\n","        M_mod[i,j] = M_mod[i,j] / 20\n","        M_mod[j,i] = M_mod[i,j]\n","        \n","      \n","\n","    \n","    # Searching for similar fragments, extracting the similar fragments and building the matrix R\n","    b = 0.75\n","    Bool = (M_mod < M_mod.mean(axis=1) - b * M_mod.std(axis=1))\n","    for i in range(0, fragmentNumbers):\n","      for j in range(i+1, fragmentNumbers):\n","        pointI = i+10 # Center of the fragment\n","        pointJ = j+10\n","        if Bool[i,j]:\n","          if A[pointI,pointJ] != 0 and A[pointI,pointJ] != 1 :\n","            R[pointI,pointJ] = A[pointI,pointJ]\n","          elif A[pointI,pointJ] == 0:\n","            R[pointI,pointJ] = math.exp(-np.linalg.norm(data.iloc[pointI,:nbcol] - data.iloc[pointJ,:nbcol]))\n","          R[pointJ,pointI] = R[pointI,pointJ]\n","\n","\n","\n","  else: # We compute only the adjacent temporal neighbourhood matrix\n","    \n","\n","    # Handling the intermediate data points for adjacent neighbours\n","    for i in range(1,end):\n","      A[i,i-1] =  math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i-1,:nbcol]))\n","      A[i-1,i] = A[i,i-1]\n","      A[i,i+1] = math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i+1,:nbcol]))\n","      A[i+1,i] = A[i,i+1]\n","  \n","  #to_drop = [\"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"]\n","  data.drop(data.iloc[:, dim:nbcol], axis=1, inplace=True)\n","  #data.drop(columns=to_drop, inplace=True)\n","  data.iloc[:,:dim] = spectral_embedding(A+R,dim)\n","  return data\n","\n","\n","\n","def dimensionReduction(data, nbcol, dim=3):\n","  #data = data.dropna()\n","  group = data.groupby(\"Series\")\n","  reducedData = group.apply(reduce, nbcol, dim)\n","  return reducedData\n","\n","train2 = train\n","test2 = test\n","\n","dim = 3\n","\n","train_r = dimensionReduction(train, nb_col, dim)\n","print(train_r)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              3         4         5  classe  Series\n","0      0.056261  0.055056 -0.054789       1       1\n","1      0.056259  0.055049 -0.054774       1       1\n","2      0.056255  0.055031 -0.054734       1       1\n","3      0.056246  0.054997 -0.054658       1       1\n","4      0.056235  0.054954 -0.054561       1       1\n","...         ...       ...       ...     ...     ...\n","63995  0.055687 -0.055613 -0.055896       4     160\n","63996  0.055702 -0.055677 -0.056039       4     160\n","63997  0.055710 -0.055710 -0.056113       4     160\n","63998  0.055716 -0.055731 -0.056160       4     160\n","63999  0.055717 -0.055738 -0.056175       4     160\n","\n","[64000 rows x 5 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aXyUhC0kKPuQ","colab_type":"code","colab":{}},"source":["# Quatre voisins\n","\n","def reduce(data, nbcol, dim=3, repetition=True):\n","  # data is a dataframe representing a serie of the dataset\n","  size = data.shape[0]\n","    \n","  # If the number or data points if less or equal to the reduced dimension wished, drop the series\n","  if dim >= size:\n","    data.drop(data.index, inplace=True)\n","    return data\n","  #if size > 25:\n","  #  repetition = True\n","    \n","\n","  A = np.zeros((size, size))\n","  np.fill_diagonal(A, 1) \n","  R = np.zeros((size, size)) # Repetition neighbourhood matrix\n","  \n","\n","  # Handling the extreme data points\n","  A[0,1] = math.exp(-np.linalg.norm(data.iloc[0,:nbcol] - data.iloc[1,:nbcol]))\n","  A[1,0] = A[0,1]\n","  end = size - 1 # Last index of A\n","  bend = end - 1\n","  A[end,bend] = math.exp(-np.linalg.norm(data.iloc[end,:nbcol] - data.iloc[bend,:nbcol]))\n","  A[bend,end] = A[end,bend]\n","\n","  if repetition == True: # We compute simultaneously the adjacent temporal neighbourhood matrix and the repetition temporal neighbourhood matrix\n","\n","    endRepetition = end-10 # Last index of the data points while computing the repetition neighbourhoods\n","    fragmentNumbers = endRepetition - 9 # Number of fragments\n","    M = np.zeros((fragmentNumbers, fragmentNumbers)) # Similarity matrix of the fragments\n","    fragments = list() # List containing the fragments. Each element of the list contains the coordinates (extracted dataframe) of the data points in the fragment\n","    M_mod = np.zeros((fragmentNumbers, fragmentNumbers)) # Similarity matrix M after windowing\n","\n","    # Handling the intermediate data points for adjacent neighbours\n","    for i in range(1,end):\n","      A[i,i-1] =  math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i-1,:nbcol]))\n","      A[i-1,i] = A[i,i-1]\n","      A[i,i+1] = math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i+1,:nbcol]))\n","      A[i+1,i] = A[i,i+1]\n","      if i >= 10 and i <= endRepetition:\n","        fragments.append(data.iloc[i-10:i+11, :nbcol])\n","\n","    # Computing the M matrix\n","    for i in range(0,fragmentNumbers):\n","      for j in range(0,i):\n","        M[i,j] = dtw(fragments[i], fragments[j])\n","        M[j,i] = M[i,j]\n","\n","    # Performing temporal windowing of M\n","    for i in range(0,fragmentNumbers):\n","      for j in range(i+1, fragmentNumbers):\n","        for b in range(0, 20):\n","          if i-b > 0 and j-b >0:  #  ET LES AUTRES ?\n","            M_mod[i,j] = M_mod[i,j] + M[i-b,j-b]\n","        M_mod[i,j] = M_mod[i,j] / 20\n","        M_mod[j,i] = M_mod[i,j]\n","        \n","      \n","\n","    \n","    # Searching for similar fragments, extracting the similar fragments and building the matrix R\n","    b = 0.75\n","    Bool = (M_mod < M_mod.mean(axis=1) - b * M_mod.std(axis=1))\n","    for i in range(0, fragmentNumbers):\n","      for j in range(i+1, fragmentNumbers):\n","        pointI = i+10 # Center of the fragment\n","        pointJ = j+10\n","        if Bool[i,j]:\n","          if A[pointI,pointJ] != 0 and A[pointI,pointJ] != 1 :\n","            R[pointI,pointJ] = A[pointI,pointJ]\n","          elif A[pointI,pointJ] == 0:\n","            R[pointI,pointJ] = math.exp(-np.linalg.norm(data.iloc[pointI,:nbcol] - data.iloc[pointJ,:nbcol]))\n","          R[pointJ,pointI] = R[pointI,pointJ]\n","\n","\n","\n","  else: # We compute only the adjacent temporal neighbourhood matrix\n","    \n","\n","    # Handling the intermediate data points for adjacent neighbours\n","    for i in range(2,end-1):\n","      A[i,i-1] =  math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i-1,:nbcol]))\n","      A[i-1,i] = A[i,i-1]\n","      A[i,i-2] =  math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i-2,:nbcol]))\n","      A[i-2,i] = A[i,i-2]\n","      A[i,i+1] = math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i+1,:nbcol]))\n","      A[i+1,i] = A[i,i+1]\n","      A[i,i+2] =  math.exp(-np.linalg.norm(data.iloc[i,:nbcol] - data.iloc[i+2,:nbcol]))\n","      A[i+2,i] = A[i,i+2]\n","  \n","  #to_drop = [\"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"]\n","  data.drop(data.iloc[:, dim:nbcol], axis=1, inplace=True)\n","  #data.drop(columns=to_drop, inplace=True)\n","  data.iloc[:,:dim] = spectral_embedding(A+R,dim)\n","  return data\n","\n","\n","\n","def dimensionReduction(data, nbcol, dim=3):\n","  #data = data.dropna()\n","  group = data.groupby(\"Series\")\n","  reducedData = group.apply(reduce, nbcol, dim)\n","  return reducedData\n","\n","train2 = train\n","test2 = test\n","\n","dim = 2\n","\n","train_r = dimensionReduction(train, nb_col, dim)\n","print(train_r)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"60JKS0P61Oi8","colab":{}},"source":["test_r = dimensionReduction(test, nb_col, dim)\n","train = train2\n","test = test2\n","print(test_r)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FFQLVuV-BLrx"},"source":["## Corr coeff"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SZI8HT2PBLrx","colab":{}},"source":["start = timer()\n","\n","\n","nb_col = len(train_r.drop(['classe','Series'],axis=1).columns)\n","nb_col\n","\n","# Calcul de la dérivée et de la somme cumulée\n","deriv = train_r.groupby('Series').transform(lambda x: x.diff()).iloc[:,0:nb_col]\n","\n","cumsum = train_r.groupby('Series').transform(lambda x: x.cumsum()).iloc[:,0:nb_col]\n","\n","data = pd.concat([train_r.iloc[:,0:nb_col],deriv,cumsum],axis=1)\n","data.columns = np.linspace(0,3*nb_col,3*nb_col,dtype=int)\n","\n","train_r = pd.concat([data,train_r[['classe','Series']]], axis=1)\n","train_r.dropna(inplace=True)\n","\n","#print(train_r)\n","\n","deriv = test_r.groupby('Series').transform(lambda x: x.diff()).iloc[:,0:nb_col]\n","\n","cumsum = test_r.groupby('Series').transform(lambda x: x.cumsum()).iloc[:,0:nb_col]\n","\n","data = pd.concat([test_r.iloc[:,0:nb_col],deriv,cumsum],axis=1)\n","data.columns = np.linspace(0,3*nb_col,3*nb_col,dtype=int)\n","\n","test_r = pd.concat([test_r.iloc[:,0:nb_col],deriv,cumsum,test_r[['classe','Series']]],\n","          axis=1)\n","test_r.dropna(inplace=True)\n","\n","#print(test_r)\n","\n","\n","####### M-histogramme #########\n","\n","# Paramètres\n","max_comb = comb(nb_col,2)*3*2\n","\n","\n","print(\"Nombre de M-histogrammes : \",max_comb)\n","\n","eval_int = 0\n","# Apprentissage vue\n","for app in range(0,4):\n","    print(\"*****************************\")\n","    print(\"Apprentissage: \",app)\n","    print(\"*****************************\")\n","    train_nngrams = list() #Liste des M-Histogrammes de l'ensemble d'apprentissage pour chaque vue\n","    test_nngrams = list() #Liste des M-Histogrammes de l'ensemble de test\n","    learners = list()\n","    combin=list()           #Liste des combinaisons dim1+dim2+TypeHistogramme\n","    \n","    # Apprentissage M-histogramme\n","    i=0\n","    while i <nb_features and i<max_comb :\n","        #if i%5==0:\n","            #print(\"*****************************\")\n","            #print(\"TOUR : \",i)\n","            #print(\"*****************************\")\n","        # Choosing : Dimensional features ||  Simple, Deriv or Cumsum ||  Bigrams or 1grams \n","        dim1rand = np.random.randint(0,nb_col)\n","        dim2rand = np.random.randint(0,nb_col)\n","        while dim2rand == dim1rand :\n","            dim2rand = np.random.randint(0,nb_col)\n","\n","\n","        transform = np.random.randint(0,3)\n","        dim1 = dim1rand+transform*nb_col\n","        dim2 = dim2rand+transform*nb_col\n","\n","        nngrams = np.random.randint(0,2)    \n","\n","        #print(\"Dim1 : \",dim1,\"   Dim2 : \",dim2,\"    NNGrams: \",nngrams)\n","\n","        if [dim1,dim2,nngrams] in combin or [dim2,dim1,nngrams] in combin : \n","            continue\n","        else :\n","            combin.append([dim1,dim2,nngrams]) \n","            i+=1\n","\n","        if nngrams == 0:\n","\n","            ub1,ub2 = bigramsTransform(train_r.copy(),dim1,dim2)\n","\n","            train_group = train_r.groupby('Series')        \n","            htr = train_group.apply(lambda x : np.ndarray.tolist(np.ndarray.flatten(\n","                        np.histogram2d(x.iloc[:,dim1].values,x.iloc[:,dim2].values,bins=[ub1,ub2],density=True)[0])))\n","            htr = pd.DataFrame(htr.values.tolist())\n","            train_nngrams.append(htr)\n","\n","\n","            test_group = test_r.groupby('Series')\n","            htt = test_group.apply(lambda x : np.ndarray.tolist(np.ndarray.flatten(\n","                        np.histogram2d(x.iloc[:,dim1].values,x.iloc[:,dim2].values,bins=[ub1,ub2],density=True)[0])))\n","            htt = pd.DataFrame(htt.values.tolist())\n","            test_nngrams.append(htt)\n","\n","            learners.append(('clf',KNeighborsClassifier(1)))\n","\n","\n","        else :\n","\n","            ub1h,ub2h = onegramsTransform(train_r.copy(),dim1,dim2) \n","\n","            train_group = train_r.groupby('Series')\n","            h1htr = train_group.apply(lambda x : np.ndarray.tolist(np.ndarray.flatten(\n","                        np.histogram(x.iloc[:,dim1].values,bins=ub1h,density=True)[0])))\n","            h1htr = pd.DataFrame(h1htr.values.tolist())\n","            train_nngrams.append(h1htr)\n","\n","            h2htr = train_group.apply(lambda x : np.ndarray.tolist(np.ndarray.flatten(\n","                        np.histogram(x.iloc[:,dim2].values,bins=ub2h,density=True)[0])))\n","            h2htr = pd.DataFrame(h2htr.values.tolist())\n","            train_nngrams.append(h2htr)\n","\n","\n","            test_group = test_r.groupby('Series')\n","            h1htt = test_group.apply(lambda x : np.ndarray.tolist(np.ndarray.flatten(\n","                        np.histogram(x.iloc[:,dim1].values,bins=ub1h,density=True)[0])))\n","            h1htt = pd.DataFrame(h1htt.values.tolist())\n","            test_nngrams.append(h1htt)\n","\n","            h2htt = test_group.apply(lambda x : np.ndarray.tolist(np.ndarray.flatten(\n","                        np.histogram(x.iloc[:,dim2].values,bins=ub2h,density=True)[0])))\n","            h2htt = pd.DataFrame(h2htt.values.tolist())\n","            test_nngrams.append(h2htt)\n","\n","            learners.append(('clf',KNeighborsClassifier(1)))\n","            learners.append(('clf',KNeighborsClassifier(1)))\n","\n","    # Score en validation\n","    cl = train_group['classe'].apply(lambda x : x.iloc[0]).reset_index(drop=True) #Liste des classes pour chaque série, en ordre\n","    se = np.arange(len(train_nngrams[0]))\n","    #train_index : index des séries de l'ensemble d'apprentissage, y_train: classes des séries de l'ensemble d'apprentissage, indexées par les index des séries\n","    train_index, test_index, y_train, y_test = train_test_split(se, cl,stratify=cl,test_size=0.2) \n","\n","    x_train = list()\n","    x_test = list()\n","    for j in range(len(train_nngrams)): #Chaque item de x_train contiendra pour une vue la liste des histogrammes des séries d'entraînement\n","        x_train.append(train_nngrams[j].iloc[train_index,:].reset_index(drop=True))  \n","    for j in range(len(train_nngrams)):\n","        x_test.append(train_nngrams[j].iloc[test_index,:].reset_index(drop=True))\n","\n","    fitted_estimators, label_encoder = el.fit_multiple_estimators(learners, x_train, y_train.reset_index(drop=True))\n","    y_pred = el.predict_from_multiple_estimator(fitted_estimators, label_encoder, x_test)\n","    score_val = np.round(accuracy_score(y_pred, y_test.reset_index(drop=True)),4)\n","    print(\"SCORE Training VUE N°\",app,\" : \",score_val )\n","    \n","    # Score en test\n","    train_group = train_r.groupby('Series')\n","    train_classe = train_group['classe'].apply(lambda x : x.iloc[0]).reset_index(drop=True)\n","\n","    test_group = test_r.groupby('Series')\n","    test_classe = test_group['classe'].apply(lambda x : x.iloc[0]).reset_index(drop=True)\n","\n","    fitted_estimators, label_encoder = el.fit_multiple_estimators(learners, train_nngrams, train_classe)\n","    y_pred = el.predict_from_multiple_estimator(fitted_estimators, label_encoder, test_nngrams)\n","    \n","    score_test = np.round(accuracy_score(y_pred, test_classe),4)\n","    print(\"SCORE TEST: \", score_test)\n","  \n","    \n","    if eval_int < score_val :\n","        eval_int=score_val\n","        eval_fi=score_test\n","        \n","print(\"SCORE FINAL: \", eval_fi)\n","\n","end = timer()\n","print(end - start)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkortF-JKPuf","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}